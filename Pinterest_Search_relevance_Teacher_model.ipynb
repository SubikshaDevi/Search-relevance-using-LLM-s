{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68WwVXkoIhhl",
        "outputId": "4802874c-b7bc-4e95-814e-9dc3bbeb19f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "https://microsoft.github.io/msmarco/Datasets.html\n",
        "\n",
        "Document Ranking dataset\n",
        "The document ranking dataset is based on source documents, which contained passages in the passage task. Although we have an incomplete set of documents that was gathered some time later than the passage data, the corpus is 3.2 million documents and our training set has 367,013 queries. For each training query, we map from a positive passage ID to the corresponding document ID in our 3.2 million. We do so on the assumption that a document that produced a relevant passage is usually a relevant document."
      ],
      "metadata": {
        "id": "c6XSGwrLIvBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#one of file was a text file, converted it to a csv\n",
        "\n",
        "import csv\n",
        "\n",
        "def convert_text_to_csv(text_file_path, csv_file_path, delimiter=','):\n",
        "    \"\"\"\n",
        "    Reads a text file and writes its content to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        text_file_path (str): Path to the input text file.\n",
        "        csv_file_path (str): Path to the output CSV file.\n",
        "        delimiter (str, optional): Delimiter to use in the CSV file. Defaults to ','.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(text_file_path, 'r') as infile, open(csv_file_path, 'w', newline='') as outfile:\n",
        "            reader = csv.reader(infile, delimiter='\\t')\n",
        "            writer = csv.writer(outfile, delimiter=delimiter)\n",
        "            for row in reader:\n",
        "                writer.writerow(row)\n",
        "        print(f\"Successfully converted '{text_file_path}' to '{csv_file_path}'\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: Text file '{text_file_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "text_file = '/content/drive/My Drive/dataset-search-relevance/validate/msmarco-docdev-top100'\n",
        "csv_file = '/content/drive/My Drive/dataset-search-relevance/validate/msmarco-docdev-top100.csv'\n",
        "convert_text_to_csv(text_file, csv_file)"
      ],
      "metadata": {
        "id": "-yl8LAI7IjPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data with query/ description\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "queries_file_path = '/content/drive/My Drive/dataset-search-relevance/train/msmarco-doctrain-queries.tsv'\n",
        "\n",
        "try:\n",
        "    queries_df = pd.read_csv(queries_file_path, sep='\\t', names=['query_id', 'query'])\n",
        "    print(\"Queries DataFrame loaded successfully:\")\n",
        "    print(queries_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Queries file not found at {queries_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the queries file: {e}\")"
      ],
      "metadata": {
        "id": "MeYIj2DTJ6yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training data\n",
        "'''\n",
        "  Number of records - 36,701,116\n",
        "  So chunking the data and creating csv with each chunk size =  100,000\n",
        "'''\n",
        "\n",
        "corpus_file = '/content/drive/MyDrive/dataset-search-relevance/train/msmarco-doctrain-top100.csv'  # Replace with your actual corpus filename\n",
        "output_prefix = 'top100_reader_'\n",
        "output_dir = '/content/drive/MyDrive/dataset-search-relevance/train/'  # Directory to save the output chunks\n",
        "\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "chunk_size = 100000  # Adjust chunk size as needed\n",
        "row_count = 0\n",
        "chunk_number = 1\n",
        "\n",
        "try:\n",
        "    corpus_reader = pd.read_csv(corpus_file, sep = ' ',chunksize=chunk_size, names = ['query_id','Q0', 'doc_id', 'rank','score','likelihood'])\n",
        "    # top100_reader_chunks = next(corpus_reader)\n",
        "    print(corpus_reader)\n",
        "    for corpus_chunk in corpus_reader:\n",
        "        # Remove rows from the current corpus chunk\n",
        "        remaining_chunk = remove_first_chunk_rows_multi_id(corpus_chunk, first_chunk_df,['query_id','Q0','rank','score','likelihood']) # Or use the multi_id version\n",
        "\n",
        "        if not remaining_chunk.empty:\n",
        "            output_file = os.path.join(output_dir, f\"{output_prefix}{chunk_number}.csv\")\n",
        "            remaining_chunk.to_csv(output_file, index=False)\n",
        "            print(f\"Saved chunk {chunk_number} with {len(remaining_chunk)} rows to {output_file}\")\n",
        "            chunk_number += 1\n",
        "        else:\n",
        "            print(f\"Chunk {chunk_number} was empty after removing rows.\")\n",
        "            chunk_number += 1\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Corpus file not found: {corpus_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing corpus file: {e}\")"
      ],
      "metadata": {
        "id": "ajJB6yldJ60r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Joining the training data to description (source documents) to get\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def join_corpus_chunk(corpus_chunk, top100_reader_first_chunk):\n",
        "  \"\"\"Joins corpus data with top100 data on 'doc_id'.\"\"\"\n",
        "  # --- Troubleshooting Start ---\n",
        "  print(\"--- Inside join_corpus_chunk ---\")\n",
        "  print(\"top100_reader_first_chunk:\")\n",
        "  print(top100_reader_first_chunk.head())\n",
        "  print(\"corpus_chunk:\")\n",
        "  print(corpus_chunk.head())\n",
        "\n",
        "  # Check data types\n",
        "  print(f\"top100_reader_first_chunk['doc_id'] dtype: {top100_reader_first_chunk['doc_id'].dtype}\")\n",
        "  print(f\"corpus_chunk['doc_id'] dtype: {corpus_chunk['doc_id'].dtype}\")\n",
        "\n",
        "  # Check for null values\n",
        "  print(f\"Null values in top100_reader_first_chunk['doc_id']: {top100_reader_first_chunk['doc_id'].isnull().sum()}\")\n",
        "  print(f\"Null values in corpus_chunk['doc_id']: {corpus_chunk['doc_id'].isnull().sum()}\")\n",
        "\n",
        "  # Check for empty strings\n",
        "  if top100_reader_first_chunk['doc_id'].dtype == object:\n",
        "    print(f\"Empty strings in top100_reader_first_chunk['doc_id']: {(top100_reader_first_chunk['doc_id'] == '').sum()}\")\n",
        "  if corpus_chunk['doc_id'].dtype == object:\n",
        "    print(f\"Empty strings in corpus_chunk['doc_id']: {(corpus_chunk['doc_id'] == '').sum()}\")\n",
        "\n",
        "  # Check unique values\n",
        "  print(f\"Unique doc_ids in top100_reader_first_chunk: {top100_reader_first_chunk['doc_id'].nunique()}\")\n",
        "  print(f\"Unique doc_ids in corpus_chunk: {corpus_chunk['doc_id'].nunique()}\")\n",
        "\n",
        "  # Check for common values\n",
        "  common_doc_ids = pd.merge(top100_reader_first_chunk, corpus_chunk, on='doc_id', how='inner')['doc_id']\n",
        "  print(f\"Number of common doc_ids: {len(common_doc_ids)}\")\n",
        "  if (len(common_doc_ids) > 0):\n",
        "    print(f\"Example common doc_ids: {common_doc_ids.head()}\")\n",
        "\n",
        "  # --- Troubleshooting End ---\n",
        "  joined_data = pd.merge(top100_reader_first_chunk, corpus_chunk, on='doc_id', how='inner')\n",
        "  print(\"Joined data (first few rows):\")\n",
        "  print(joined_data.head())\n",
        "  return joined_data\n",
        "\n",
        "results = []\n",
        "docs_file_path = '/content/drive/My Drive/dataset-search-relevance/train/msmarco-docs.tsv'  #\n",
        "columns_names_docs = [ 'doc_id', 'url', 'title', 'body']\n",
        "count = 0\n",
        "column_names = ['query_id', 'Q0', 'doc_id', 'rank', 'score', 'likelihood']\n",
        "\n",
        "top100_reader_file_path = '/content/drive/MyDrive/dataset-search-relevance/train/top100_reader_300.csv'\n",
        "top100_reader_first_chunk = pd.read_csv(top100_reader_file_path,names = column_names)\n",
        "top100_reader_first_chunk = top100_reader_first_chunk[top100_reader_first_chunk['query_id']!= 'query_id']\n",
        "print(top100_reader_first_chunk.head())\n",
        "try:\n",
        "    docs_reader = pd.read_csv(docs_file_path, sep='\\t',names = columns_names_docs, chunksize=100000)\n",
        "    for corpus_chunk in docs_reader:\n",
        "      # if count == 15:\n",
        "      #   break\n",
        "      print(f\"Processing corpus chunk with {len(corpus_chunk)} rows\")\n",
        "      joined_chunk = join_corpus_chunk(corpus_chunk, top100_reader_first_chunk)\n",
        "      results.append(joined_chunk)  #\n",
        "      # count+=1\n",
        "    # docs_reader_first_chunk = next(docs_reader)\n",
        "    print(\"Queries DataFrame loaded successfully:\")\n",
        "    # print(docs_reader_first_chunk.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Queries file not found at {docs_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the queries file: {e}\")\n",
        "\n",
        "final_joined_df = pd.concat(results, ignore_index=True)  # Combine all the DataFrames in 'results'\n",
        "\n",
        "print(\"Final joined DataFrame:\")\n",
        "print(final_joined_df.head())\n",
        "print(final_joined_df.shape)\n"
      ],
      "metadata": {
        "id": "PZl7gKwlJ63B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_joined_df.info()\n",
        "final_joined_df_temp = final_joined_df[final_joined_df['body'].notna()] #removing null values\n",
        "\n",
        "final_joined_df_temp['query_id'] = final_joined_df_temp['query_id'].astype('int64')\n",
        "# final_joined_df_temp.info()\n",
        "\n",
        "top100_reader_first_chunk_train = pd.merge(final_joined_df_temp,queries_df, on='query_id')\n",
        "top100_reader_first_chunk_train.shape"
      ],
      "metadata": {
        "id": "cMbRaizjJ65X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the merged chunks as csv in order to avoid repetition\n",
        "\n",
        "top100_reader_first_chunk_train = top100_reader_first_chunk_train[['query','body','rank']]\n",
        "top100_reader_first_chunk_train.head()\n",
        "\n",
        "top100_reader_first_chunk_train.to_csv('/content/drive/My Drive/dataset-search-relevance/train/top100_reader_5th_chunk_train.csv')"
      ],
      "metadata": {
        "id": "AKwWL1kkL5_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top100_reader_first_chunk_train['rank'].unique()"
      ],
      "metadata": {
        "id": "nwLOycdsL6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validation set - same preprocessing\n",
        "\n",
        "validation_path_queries = '/content/drive/My Drive/dataset-search-relevance/validate/msmarco-docdev-queries.tsv'\n",
        "validation_path_top100 = '/content/drive/My Drive/dataset-search-relevance/validate/msmarco-docdev-top100.csv'\n",
        "\n",
        "columns_names_query_df = ['qid', 'query']\n",
        "columns_names_top100 = ['qid', 'Q0', 'doc_id', 'rank', 'score', 'runstring']\n",
        "validation_query_df = pd.read_csv(validation_path_queries, sep='\\t',names = columns_names_query_df)\n",
        "\n",
        "validation_top100_reader = pd.read_csv(validation_path_top100, sep=' ',names = columns_names_top100)\n",
        "validation_query_df.head()"
      ],
      "metadata": {
        "id": "hZsFSpuvM6PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training distillbert model (Hugging Face model) for Search\n",
        "\n",
        "Relevance Label & Description\n",
        "\n",
        "Excellent / Highly Relevant (L5) - 4 - Exactly matches or directly associates with the search query.\n",
        "\n",
        "Good / Relevant (L4) - 3 - Close match or a potential substitute to the search query, with slight mismatches.\n",
        "\n",
        "Complementary / Marginally Relevant (L3) - 2 - Related to the search query but only partially matches it, not specifically addressing the intent.\n",
        "\n",
        "\n",
        "Poor / Irrelevant (L2) - 1 - Fitting into the general category but not serving the intended purpose or matching the user intent.\n",
        "\n",
        "\n",
        "Highly Irrelevant (L1) - 0 - Completely irrelevant to the search, potentially causing user dissatisfaction"
      ],
      "metadata": {
        "id": "PQYeFbJ7NGrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get all the training chunks\n",
        "import pandas as pd\n",
        "\n",
        "chunk1 = pd.read_csv('/content/drive/My Drive/dataset-search-relevance/train/top100_reader_5th_chunk_train.csv')\n",
        "chunk1.head()"
      ],
      "metadata": {
        "id": "TlXWCoCTM6Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for Training purposes, assigning an equal class frequency (0-4)\n",
        "chunk1['rank_class_eq_freq'] = pd.qcut(chunk1['rank'], q=5, labels=False)\n",
        "\n",
        "chunk1[['rank', 'rank_class_eq_freq']].head(10)\n",
        "chunk1['rank_class_eq_freq'].value_counts()"
      ],
      "metadata": {
        "id": "xTI8Eyy1M6UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing the training dataset\n",
        "'''\n",
        "  converting raw data to tokens for LLM's to understand\n",
        "  Making the dataset suitable for hugging face data by adding attention mask & input_ids\n",
        "  attention mask - to differentite padded values and origial data\n",
        "'''\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import gc  # For garbage collection\n",
        "import numpy as np  # Import NumPy\n",
        "\n",
        "def tokenize_chunks(top100_reader_first_chunk_train):\n",
        "  # --- Configuration ---\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "  chunksize = 700  # Adjust as needed\n",
        "  query_col = \"query\"  # Replace with your actual query column name\n",
        "  passage_col = \"body\"  # Replace with your actual passage column name\n",
        "  rank_col = \"rank_class_eq_freq\"  # Replace with your actual rank column name\n",
        "\n",
        "  # --- Safety Check: Ensure Columns Exist ---\n",
        "  if query_col not in top100_reader_first_chunk_train.columns:\n",
        "      raise ValueError(f\"Query column '{query_col}' not found in DataFrame.\")\n",
        "  if passage_col not in top100_reader_first_chunk_train.columns:\n",
        "      raise ValueError(f\"Passage column '{passage_col}' not found in DataFrame.\")\n",
        "  if rank_col not in top100_reader_first_chunk_train.columns:\n",
        "      raise ValueError(f\"Rank column '{rank_col}' not found in DataFrame.\")\n",
        "\n",
        "  # --- Tokenize in Chunks ---\n",
        "  tokenized_input_ids_list = []\n",
        "  tokenized_attention_masks_list = []\n",
        "  original_queries = []\n",
        "  original_bodies = []\n",
        "  original_ranks = []\n",
        "\n",
        "  for i in range(0, len(top100_reader_first_chunk_train), chunksize):\n",
        "      df_chunk = top100_reader_first_chunk_train.iloc[i:i + chunksize].copy() # Explicitly create a copy\n",
        "\n",
        "      # Force string conversion and handle potential NaNs\n",
        "      query_texts = df_chunk[query_col].astype(str).fillna(\"\").tolist()\n",
        "      passage_texts = df_chunk[passage_col].astype(str).fillna(\"\").tolist()\n",
        "\n",
        "      tokenized_data = tokenizer(\n",
        "          query_texts,\n",
        "          passage_texts,\n",
        "          truncation=True,\n",
        "          padding=True,\n",
        "          max_length=128,  # Limit sequence length\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "      tokenized_input_ids_list.append(tokenized_data[\"input_ids\"])\n",
        "      tokenized_attention_masks_list.append(tokenized_data[\"attention_mask\"])\n",
        "      original_queries.extend(df_chunk[query_col].astype(str).tolist())\n",
        "      original_bodies.extend(df_chunk[passage_col].astype(str).tolist())\n",
        "      original_ranks.extend(df_chunk[rank_col].tolist())\n",
        "\n",
        "      print(f\"Tokenized chunk {i // chunksize + 1}/{len(top100_reader_first_chunk_train) // chunksize + 1}\")\n",
        "\n",
        "      del df_chunk, query_texts, passage_texts, tokenized_data\n",
        "      gc.collect()\n",
        "\n",
        "  # --- Concatenate Tokenized Data ---\n",
        "  tokenized_input_ids = torch.cat(tokenized_input_ids_list)\n",
        "  tokenized_attention_masks = torch.cat(tokenized_attention_masks_list)\n",
        "\n",
        "  # --- Create Hugging Face Dataset ---\n",
        "  dataset_dict = {\n",
        "      \"query\": original_queries,\n",
        "      \"body\": original_bodies,\n",
        "      \"rank_class_eq_freq\": original_ranks,\n",
        "      \"input_ids\": tokenized_input_ids.numpy().tolist(),  # Convert directly to NumPy then list\n",
        "      \"attention_mask\": tokenized_attention_masks.numpy().tolist(),\n",
        "  }\n",
        "\n",
        "  tokenized_dataset = Dataset.from_dict(dataset_dict)\n",
        "  tokenized_dataset.set_format(\"torch\")  # Set to PyTorch format\n",
        "\n",
        "  print(tokenized_dataset)\n",
        "  # print(tokenized_dataset[0])\n",
        "\n",
        "  return tokenized_dataset"
      ],
      "metadata": {
        "id": "ylbM0aX7QWMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenize_chunks(chunk1)\n",
        "save_path = '/content/drive/My Drive/dataset-search-relevance/train/my_tokenized_dataset4'  # Replace with your actual path\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "tokenized_dataset.save_to_disk(save_path)\n",
        "\n",
        "print(f\"Tokenized dataset saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "Knz3GNYqM6Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Using Hugging face model - distilbert-base-uncased\n",
        "  distilbert-base-uncased is a powerful and popular pre-trained language model, a \"distilled\" (smaller and faster) version of Google's BERT (Bidirectional Encoder Representations from Transformers) model.\n",
        "  It's widely used in Natural Language Processing (NLP) tasks.\n",
        "'''\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 5  # Adjust based on your relevance scale\n",
        "model_name = \"distilbert-base-uncased\" # Replace with your chosen model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "CEq4m6ioSlfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import gc  # For garbage collection\n",
        "import numpy as np  # Import NumPy\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# 0. Load your pre-trained model\n",
        "model_file_path = '/content/drive/My Drive/dataset-search-relevance/my_trained_model_updated_epoch_20'  # Replace with your actual path\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_file_path) #  Corrected line\n",
        "# model = TheModelClass(*args, **kwargs) # Your model class\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# 1. Load your tokenized data\n",
        "saved_dataset_path = \"/content/drive/MyDrive/dataset-search-relevance/train/my_tokenized_dataset\"  # Replace with your actual path\n",
        "loaded_tokenized_dataset = load_from_disk(saved_dataset_path)\n",
        "\n",
        "# 2. Define optimizer and scheduler\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Example\n",
        "num_epochs = 3  # Adjust as needed\n",
        "num_training_steps = num_epochs * (len(loaded_tokenized_dataset) // 16) #changed\n",
        "if len(loaded_tokenized_dataset) % 16 != 0:\n",
        "    num_training_steps += num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=num_training_steps)\n",
        "\n",
        "# 3. TensorBoard Writer\n",
        "tb_writer = SummaryWriter(log_dir='/content/drive/My Drive/dataset-search-relevance/runs') #log dir\n",
        "\n",
        "# 4. Continue training\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "model.train()  # Put model in training mode\n",
        "\n",
        "# 1. Create DataLoader from the pre-loaded dataset\n",
        "input_ids = torch.as_tensor(loaded_tokenized_dataset[\"input_ids\"].numpy()) # change\n",
        "attention_mask = torch.as_tensor(loaded_tokenized_dataset[\"attention_mask\"].numpy()) # change\n",
        "labels = torch.as_tensor(loaded_tokenized_dataset[\"rank_class_eq_freq\"].numpy())  # Assuming you have 'labels' #change\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 2. Training Loop with Monitoring\n",
        "best_eval_loss = float('inf')  # Initialize for early stopping\n",
        "patience = 10  # Number of epochs to wait for improvement\n",
        "no_improvement_count = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n",
        "    tb_writer.add_scalar(\"Loss/train\", avg_loss, epoch)  # Log\n",
        "\n",
        "    # 3. Evaluation (for monitoring overfitting)\n",
        "    model.eval()  # Put model in evaluation mode\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:  # Use the same dataloader\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
        "            outputs = model(**inputs)\n",
        "            eval_loss += outputs.loss.item()\n",
        "    avg_eval_loss = eval_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Evaluation Loss: {avg_eval_loss}\")\n",
        "    tb_writer.add_scalar(\"Loss/eval\", avg_eval_loss, epoch)  # Log\n",
        "\n",
        "    model.train()  # Put back in train mode\n",
        "\n",
        "    # 4. Early Stopping (example, adjust as needed)\n",
        "    if avg_eval_loss < best_eval_loss:\n",
        "        best_eval_loss = avg_eval_loss\n",
        "        no_improvement_count = 0\n",
        "        # Save the best model so far\n",
        "        model_file_path = '/content/drive/My Drive/dataset-search-relevance/best_model_trained'\n",
        "        model.save_pretrained(model_file_path)\n",
        "        # torch.save(model.state_dict(), model_file_path)\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "        if no_improvement_count >= patience:\n",
        "            print(f\"Early stopping: No improvement in evaluation loss for {patience} epochs.\")\n",
        "            break\n",
        "\n",
        "# 5. Save the Trained Model\n",
        "model_file_path = '/content/drive/My Drive/dataset-search-relevance/my_trained_model_updated_epoch_10'  # Replace with your actual path\n",
        "model.save_pretrained(model_file_path)  # save state dict\n",
        "# tokenizer_file_path = '/content/drive/My Drive/dataset-search-relevance/tokenizer'  # Replace with your actual path\n",
        "# tokenizer.save_pretrained(tokenizer_file_path)\n",
        "tb_writer.close()  # close\n",
        "\n",
        "# continue_training(model, loaded_tokenized_dataset, num_epochs, optimizer, scheduler, tb_writer)\n",
        "# print(\"Training completed and model saved.\")\n"
      ],
      "metadata": {
        "id": "z4_byTuTSllv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1 Average Loss: 1.1895894569851821\n",
        "\n",
        "Epoch 1 Evaluation Loss: 0.9516131887436734\n",
        "\n",
        "Epoch 2 Average Loss: 0.9926541730176968\n",
        "\n",
        "Epoch 2 Evaluation Loss: 0.709671673389935\n",
        "\n",
        "Epoch 3 Average Loss: 0.7654313624971748\n",
        "\n",
        "Epoch 3 Evaluation Loss: 0.5223395917654365"
      ],
      "metadata": {
        "id": "91ULFdfMTW6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation for DistillBERT model"
      ],
      "metadata": {
        "id": "nQUwEuQ2SHuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#validation dataset - distilbert\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import gc  # For garbage collection\n",
        "\n",
        "# --- Configuration ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "chunksize = 700  # Start with a VERY small chunksize! Adjust upwards cautiously\n",
        "query_col = \"query\"  # Replace with your actual query column name\n",
        "passage_col = \"body\"  # Replace with your actual passage column name\n",
        "rank_col = \"rank_class_eq_freq\"  # Replace with your actual rank column name\n",
        "\n",
        "# --- Safety Check: Ensure Columns Exist ---\n",
        "if query_col not in validation_df.columns:\n",
        "    raise ValueError(f\"Query column '{query_col}' not found in DataFrame.\")\n",
        "if passage_col not in validation_df.columns:\n",
        "    raise ValueError(f\"Passage column '{passage_col}' not found in DataFrame.\")\n",
        "if rank_col not in validation_df.columns:\n",
        "    raise ValueError(f\"Rank column '{rank_col}' not found in DataFrame.\")\n",
        "\n",
        "# --- Tokenize in Very Small Chunks ---\n",
        "tokenized_input_ids = []\n",
        "tokenized_attention_masks = []\n",
        "original_queries = []\n",
        "original_bodies = []\n",
        "original_ranks = []\n",
        "\n",
        "for i in range(0, len(validation_df), chunksize):\n",
        "    df_chunk = validation_df.iloc[i:i + chunksize]\n",
        "\n",
        "    # Force string conversion and handle potential NaNs\n",
        "    query_texts = df_chunk[query_col].astype(str).fillna(\"\").tolist()\n",
        "    passage_texts = df_chunk[passage_col].astype(str).fillna(\"\").tolist()\n",
        "\n",
        "    tokenized_data = tokenizer(\n",
        "        query_texts,\n",
        "        passage_texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,  # Limit sequence length\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    tokenized_input_ids.append(tokenized_data[\"input_ids\"])\n",
        "    tokenized_attention_masks.append(tokenized_data[\"attention_mask\"])\n",
        "    original_queries.extend(df_chunk[query_col].astype(str).tolist())  # Keep original data\n",
        "    original_bodies.extend(df_chunk[passage_col].astype(str).tolist())\n",
        "    original_ranks.extend(df_chunk[rank_col].tolist())\n",
        "\n",
        "    print(f\"Tokenized chunk {i // chunksize + 1}/{len(validation_df) // chunksize + 1}\")\n",
        "\n",
        "    # Explicitly delete and garbage collect\n",
        "    del df_chunk, query_texts, passage_texts, tokenized_data\n",
        "    gc.collect()\n",
        "\n",
        "# --- Concatenate Tokenized Data ---\n",
        "tokenized_input_ids = torch.cat(tokenized_input_ids)\n",
        "tokenized_attention_masks = torch.cat(tokenized_attention_masks)\n",
        "\n",
        "# --- Create Hugging Face Dataset ---\n",
        "dataset_dict = {\n",
        "    \"query\": original_queries,\n",
        "    \"body\": original_bodies,\n",
        "    \"rank_class_eq_freq\": original_ranks,\n",
        "    \"input_ids\": tokenized_input_ids.tolist(),  # Convert tensors to lists\n",
        "    \"attention_mask\": tokenized_attention_masks.tolist(),\n",
        "}\n",
        "\n",
        "tokenized_validation_dataset = Dataset.from_dict(dataset_dict)\n",
        "tokenized_validation_dataset.set_format(\"torch\")  # Set to PyTorch format\n",
        "\n",
        "print(tokenized_validation_dataset)\n",
        "print(tokenized_validation_dataset[0])\n",
        "\n",
        "save_path = '/content/drive/My Drive/dataset-search-relevance/validate/my_tokenized_validation_dataset'  # Replace with your actual path\n",
        "\n",
        "\n",
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "tokenized_validation_dataset.save_to_disk(save_path)\n",
        "\n",
        "print(f\"Tokenized dataset saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "kAkaSkOjR9uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_file_path) # Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_file_path) # Load tokenizer\n",
        "model.to(device)\n",
        "model.eval()  # Put the model in evaluation mode\n",
        "\n",
        "def predict_relevance(query, passage):\n",
        "    tokenized_input = tokenizer(query, passage, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_input)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1)\n",
        "    return predicted_class.item()  # Or process logits for ranking\n",
        "\n",
        "# Example Usage\n",
        "query = \"best running shoes\"\n",
        "passage = \"Nike shoes\"\n",
        "relevance = predict_relevance(query, passage)\n",
        "print(f\"Relevance of passage to query: {relevance}\")"
      ],
      "metadata": {
        "id": "fcD3ZDULR9xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevance of passage to query: 4"
      ],
      "metadata": {
        "id": "1u0jLb3xTj-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final distilbert model validation\n",
        "\n",
        "model_file_path = '/content/drive/My Drive/dataset-search-relevance/my_trained_model_updated'\n",
        "tokenizer_file_path = '/content/drive/My Drive/dataset-search-relevance/tokenizer'\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_file_path) # Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_file_path) # Load tokenizer\n",
        "model.to(device)\n",
        "model.eval()  # Put the model in evaluation mode\n",
        "\n",
        "def predict_relevance(query, passage):\n",
        "    tokenized_input = tokenizer(query, passage, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_input)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1)\n",
        "    return predicted_class.item()  # Or process logits for ranking\n",
        "\n",
        "# Example Usage\n",
        "query = \"best running shoes\"\n",
        "passage = \"Nike is abrand built for shoes. they make the bext kind of shoes ever\"\n",
        "relevance = predict_relevance(query, passage)\n",
        "print(f\"Relevance of passage to query: {relevance}\")"
      ],
      "metadata": {
        "id": "KLiEA5MFR90a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevance of passage to query: 2"
      ],
      "metadata": {
        "id": "_nb5gvHfTp6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training RoBERTa model\n",
        "\n",
        "RoBERTa is a transformer-based language model developed by Facebook AI. It's an improved version of Google's BERT (Bidirectional Encoder Representations from Transformers) model."
      ],
      "metadata": {
        "id": "qHJueLgzT7L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_from_disk\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard  # Import TensorBoard\n",
        "import os\n",
        "def create_dataloader(tokenized_dataset, batch_size):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader from a tokenized dataset.\n",
        "    Args:\n",
        "        tokenized_dataset: A Hugging Face Dataset object.\n",
        "        batch_size: The batch size.\n",
        "    \"\"\"\n",
        "    # tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"rank_class_eq_freq\"]) #set format\n",
        "    tokenized_dataset = tokenized_dataset.rename_column(\"rank_class_eq_freq\", \"labels\")\n",
        "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load your tokenized dataset (replace with your actual path)\n",
        "    saved_dataset_path = \"/content/drive/MyDrive/dataset-search-relevance/train/my_tokenized_dataset\"\n",
        "    tokenized_dataset = load_from_disk(saved_dataset_path)\n",
        "\n",
        "    # 2. Load the tokenizer (same as before)\n",
        "    tokenizer_name = \"distilbert-base-uncased\"  # Use the same tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    # 3. Create the DataLoader\n",
        "    batch_size = 16\n",
        "    dataloader = create_dataloader(tokenized_dataset, batch_size)\n",
        "\n",
        "    # 4. Load a *different* LLM, but with the same tokenizer's vocabulary\n",
        "    new_model_name = \"roberta-base\"  # Example: Change to the new model you want\n",
        "    num_labels = 5 # add num_labels\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(new_model_name, num_labels=num_labels)\n",
        "\n",
        "    # 5. Set up training parameters\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    num_epochs = 3  # Adjust as needed\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)  # Example learning rate\n",
        "    num_training_steps = num_epochs * len(dataloader)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=num_training_steps)\n",
        "    # tb_writer = SummaryWriter(log_dir='/content/drive/runs_roberta')  # TensorBoard log directory\n",
        " # TensorBoard log directory\n",
        "\n",
        "    # 6. Training Loop with Monitoring and Early Stopping\n",
        "    best_eval_loss = float('inf')  # Initialize for early stopping\n",
        "    patience = 3  # Number of epochs to wait for improvement\n",
        "    no_improvement_count = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        model.train()  # Put model in training mode\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()} #send batch to device\n",
        "            # print(batch.keys())\n",
        "            # inputs = {k: v for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'labels']} #filter the batch\n",
        "            # outputs = model(**inputs)\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n",
        "        # tb_writer.add_scalar(\"Loss/train\", avg_loss, epoch)  # Log\n",
        "\n",
        "        # 7. Evaluation (for monitoring overfitting) - VERY IMPORTANT\n",
        "        model.eval()  # Put model in evaluation mode\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                eval_loss += outputs.loss.item()\n",
        "        avg_eval_loss = eval_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1} Evaluation Loss: {avg_eval_loss}\")\n",
        "        # tb_writer.add_scalar(\"Loss/eval\", avg_eval_loss, epoch)  # Log\n",
        "\n",
        "        model.train() #put model back to train\n",
        "\n",
        "        # 8. Early Stopping (example, adjust as needed)\n",
        "        if avg_eval_loss < best_eval_loss:\n",
        "            best_eval_loss = avg_eval_loss\n",
        "            no_improvement_count = 0\n",
        "            # Save the best model so far\n",
        "            model_file_path = '/content/drive/My Drive/dataset-search-relevance/best_model_roberta'\n",
        "            model.save_pretrained(model_file_path)\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= patience:\n",
        "                print(\n",
        "                    f\"Early stopping: No improvement in evaluation loss for {patience} epochs.\")\n",
        "                break\n",
        "\n",
        "    # 9. Save the Trained Model\n",
        "    model_file_path = '/content/drive/My Drive/dataset-search-relevance/my_trained_model_roberta'  # Replace with your actual path\n",
        "    model.save_pretrained(model_file_path)\n",
        "    # tb_writer.close()  # close\n"
      ],
      "metadata": {
        "id": "-TQEdGFhT6rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Epoch 1 Average Loss: 1.6135458814958334\n",
        "\n",
        "Epoch 1 Evaluation Loss: 1.609668828746982\n",
        "\n",
        "Epoch 2 Average Loss: 1.6114520889598587\n",
        "\n",
        "Epoch 2 Evaluation Loss: 1.6100000911389067\n",
        "\n",
        "Epoch 3 Average Loss: 1.6104497203971302\n",
        "\n",
        "Epoch 3 Evaluation Loss: 1.6094485446246674"
      ],
      "metadata": {
        "id": "R0zADebwUNc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation for RoBERTa model"
      ],
      "metadata": {
        "id": "xdFc3iR_UYac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final roberta model validation\n",
        "'''\n",
        "As it can be seen it's not the best trained model\n",
        "'''\n",
        "\n",
        "model_file_path = '/content/drive/My Drive/dataset-search-relevance/my_trained_model_roberta'\n",
        "tokenizer_file_path = '/content/drive/My Drive/dataset-search-relevance/tokenizer'\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_file_path) # Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_file_path) # Load tokenizer\n",
        "model.to(device)\n",
        "model.eval()  # Put the model in evaluation mode\n",
        "\n",
        "def predict_relevance(query, passage):\n",
        "    tokenized_input = tokenizer(query, passage, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_input)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1)\n",
        "    return predicted_class.item()  # Or process logits for ranking\n",
        "\n",
        "# Example Usage\n",
        "query = \"best running shoes\"\n",
        "passage = \"nive shoes\"\n",
        "relevance = predict_relevance(query, passage)\n",
        "print(f\"Relevance of passage to query: {relevance}\")"
      ],
      "metadata": {
        "id": "glwPC270T6eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevance of passage to query: 0"
      ],
      "metadata": {
        "id": "tsGI2X4GUeLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy & F1-score"
      ],
      "metadata": {
        "id": "1yGdqw4wU3Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "--- Starting Evaluation ---\n",
        "\n",
        "Average Validation Loss: 2.0412\n",
        "\n",
        "--- DistilBERT Model Performance ---\n",
        "Accuracy: 0.2379\n",
        "F1-Score (macro): 0.2354\n",
        "Confusion Matrix:\n",
        "[[538 628 465 204 164]\n",
        " [299 589 587 254 236]\n",
        " [248 543 610 319 284]\n",
        " [264 528 568 330 293]\n",
        " [229 548 600 360 312]]\n",
        "AUROC (macro, OVR): 0.5522\n",
        "\n",
        "--- Evaluation Complete ---\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XjsyeN45UwI7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bznk_XTJUyOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}